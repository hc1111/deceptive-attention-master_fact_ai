{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run the experiment without the impermissible tokens replace 'train.txt' with 'train_1.txt' (and likewise for dev.txt and test.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure that you're pointing towards the impermissible tokens csv path in your code e.g.         df = pd.read_csv('C:/Users/howar/Downloads/deceptive-attention-master/deceptive-attention-master/src/classification_tasks/pytorch-pretrained-BERT/examples/impermissible.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howar\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'pytorch-pretrained-BERT'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "from pytorch_pretrained_bert_local.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert_local.modeling import BertForSequenceClassification, BertConfig\n",
    "from pytorch_pretrained_bert_local.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert_local.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None, block=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        self.block = block\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    @classmethod\n",
    "    def _read_txt(cls, input_file):\n",
    "        \"\"\"Reads a text file.\"\"\"\n",
    "        return open(input_file, encoding='UTF8').readlines()\n",
    "\n",
    "class SstWikiProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the SST-binary + Wikipedia data set (sentence level).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            SstWikiProcessor._read_txt(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "    def get_dev_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            SstWikiProcessor._read_txt(os.path.join(data_dir, \"dev.txt\")), \"dev\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_test_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            SstWikiProcessor._read_txt(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"1\", \"2\",\"3\", \"4\", \"5\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            label_text = line.split('\\t', 1)\n",
    "            text_split = label_text[1].split('[SEP]')\n",
    "            text_sst = text_split[0].strip()\n",
    "            text_wiki = text_split[1].strip() \n",
    "            label = label_text[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_wiki, text_b=text_sst, label=label))\n",
    "        return examples\n",
    "\n",
    "class PronounProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Pronoun data set (sentence level).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"train.txt\")),\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"train.txt.block\")),\n",
    "            \"train\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "    def get_dev_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"dev.txt\")),\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"dev.txt.block\")),\n",
    "            \"dev\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_test_examples(self, data_dir, limit=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        ret = self._create_examples(\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"test.txt\")),\n",
    "            PronounProcessor._read_txt(os.path.join(data_dir, \"test.txt.block\")),\n",
    "            \"test\")\n",
    "        if limit:\n",
    "            ret = ret[0:limit]\n",
    "        return ret\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"1\", \"2\",\"3\", \"4\", \"5\"]\n",
    "\n",
    "    def _create_examples(self, lines, block_lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            label_text = line.split('\\t', 1)\n",
    "            if len(label_text)>1:\n",
    "                # print('index')\n",
    "                # print(i)\n",
    "                # print(len(block_lines))\n",
    "                label = label_text[0]\n",
    "                text = label_text[1]\n",
    "                block = block_lines[i]\n",
    "                examples.append(\n",
    "                    InputExample(guid=guid, text_a=text, text_b=None, label=label, block=block))\n",
    "\n",
    "                # print(block_lines)\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, output_mode):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        # print('example.block')\n",
    "        # print(example.block)\n",
    "        # print(tokens_a)\n",
    "        # segment_ids=[int(x) token in tokens_a]\n",
    "        # if example.block:\n",
    "        # print(tokens_a)\n",
    "        # segment_ids = [int(item) for item in example.block.split()]\n",
    "        # print(os.getcwd())\n",
    "        #         # # print('basbkdas')\n",
    "        # filepath=os.path.join(os.getcwd(),'../pytorch-pretrained-BERT/examples/impermissible.csv')\n",
    "        # print(filepath)\n",
    "        df = pd.read_csv('C:/Users/howar/Downloads/deceptive-attention-master/deceptive-attention-master/src/classification_tasks/pytorch-pretrained-BERT/examples/impermissible.csv')\n",
    "        # df = pd.read_csv('/home/lgpu0151/Howard/src/classification_tasks/pytorch-pretrained-BERT/examples/impermissible.csv')\n",
    "\n",
    "\n",
    "        pronoun_list=df['0'].to_list()\n",
    "        #     pronoun_list = [\"her\", \"his\", \"him\", \"she\", \"he\", \"herself\", \"himself\", \"hers\", \"mr\", \"mrs\", \"ms\", \"mr.\", \"mrs.\", \"ms.\"]\n",
    "        segment_ids = [1 if token.lower() in pronoun_list else 0 for token in tokens_a]\n",
    "        # else:\n",
    "        #     segment_ids = [0]*len(tokens_a)\n",
    "        # print(segment_ids)\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "                segment_ids = segment_ids[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] + segment_ids + [0]\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        # print(\"OOOOOOOO\", len(input_ids), len(input_mask), len(segment_ids), max_seq_length)\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            # print(label_map)\n",
    "            label_id = label_map[example.label]\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "        \"corr\": (pearson_corr + spearman_corr) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(input_processor_type, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    if input_processor_type == \"sst-wiki\" or input_processor_type == \"pronoun\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    else:\n",
    "        raise KeyError(input_processor_type)\n",
    "\n",
    "def attention_regularization_loss(attention_probs_layers, \n",
    "                                    pay_attention_mask,\n",
    "                                    pad_attention_mask,\n",
    "                                    hammer_coeff=0.0,\n",
    "                                    optimize_func='mean',\n",
    "                                    debug=False):\n",
    "    float_type = torch.FloatTensor\n",
    "    if torch.cuda.is_available():\n",
    "        float_type = torch.cuda.FloatTensor\n",
    "\n",
    "    reg_attention_mask = pay_attention_mask.unsqueeze(1).unsqueeze(2).type(float_type)\n",
    "    pad_attention_mask = (1-pad_attention_mask).unsqueeze(1).unsqueeze(2).type(float_type)\n",
    "    non_reg_attention_mask = 1 - (reg_attention_mask + pad_attention_mask)\n",
    "    # attention_probs_layers - [B x H x aW x bW] [32, 12, 128, 128]\n",
    "    # pay_attention_mask     -  B x W             32,  1,   1, 128\n",
    "    #                        -  0..., 1..., 0... - WIKI, SST, PAD\n",
    "    # minimize attention to SST words\n",
    "\n",
    "    # We are only interested in last layer, and CLS token (first token)\n",
    "    attention_probs_layer = attention_probs_layers[-1][:, :, 0, :].unsqueeze(2)\n",
    "    # 32, 12, 1, 128\n",
    "\n",
    "    reg_attention_maps     = attention_probs_layer * reg_attention_mask\n",
    "    pad_attention_maps     = attention_probs_layer * pad_attention_mask\n",
    "    non_reg_attention_maps = attention_probs_layer * non_reg_attention_mask\n",
    "    if debug:\n",
    "        print(f\"Regularized attention mask:{reg_attention_mask}\")\n",
    "        print(f\"Non-Regular attention mask:{non_reg_attention_mask}\")\n",
    "    # 32, 12, 1, 128\n",
    "    # 32, 12, 1, 128 -> 32, 12, 1\n",
    "    reg_attention_sum = torch.sum(reg_attention_maps, -1)\n",
    "    pad_attention_sum = torch.sum(pad_attention_maps, -1)\n",
    "    non_reg_attention_sum = torch.sum(non_reg_attention_maps, -1)\n",
    "    total_attention_sum=torch.sum(attention_probs_layer, -1)\n",
    "\n",
    "    if optimize_func == 'mean':\n",
    "        hammer_reg = torch.mean( torch.log(1 - reg_attention_sum) )\n",
    "    else:\n",
    "        # minimize max attention_sum\n",
    "        # minimize min log(1 - attention_sum)\n",
    "        hammer_reg = torch.min( torch.log(1 - reg_attention_sum) )\n",
    "    return - hammer_coeff * hammer_reg, torch.mean(reg_attention_sum), torch.mean(non_reg_attention_sum),reg_attention_sum, torch.mean(pad_attention_sum), torch.max(reg_attention_sum), torch.max(non_reg_attention_sum), torch.argmax(non_reg_attention_sum, dim=1)\n",
    "\n",
    "def main(params):\n",
    "    parser = argparse.ArgumentParser()\n",
    "#     parser.parse_args()\n",
    "\n",
    "    ## Required parameters\n",
    "    parser.add_argument(\"--data_dir\",\n",
    "                        default='data/twitter_bert',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str, required=False,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "    parser.add_argument(\"--input_processor_type\",\n",
    "                        default=\"pronoun\",\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        help=\"The type of processor to use for reading data.\")\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default=params['output_dir']+\"/\"+str(params['hammer_coeff'])+params['optfunc'],\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=500,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--do_train\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\",\n",
    "                        default=True,\n",
    "\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--do_lower_case\",\n",
    "                        default=True,\n",
    "\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--train_batch_size\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=2e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=4.0,\n",
    "                        type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "    parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "    parser.add_argument('--hammer_coeff',\n",
    "                        type=float,\n",
    "                        default=params['hammer_coeff'],\n",
    "                        help=\"Hammer loss coefficient\")\n",
    "    parser.add_argument('--att_opt_func',\n",
    "                        type=str,\n",
    "                        default=params['optfunc'],\n",
    "                        help=\"Attention optimization function\")\n",
    "    parser.add_argument(\"--debug\",\n",
    "                        action='store_true')\n",
    "    parser.add_argument(\"--first_run\",\n",
    "                        default=True,\n",
    "                        action='store_true')\n",
    "    parser.add_argument(\"--name\",\n",
    "                        type=str)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    \n",
    "    base_labels = {}\n",
    "    print(f\"FIRST RUN: {args.first_run}\")\n",
    "    if not args.first_run:\n",
    "        for typ in [\"dev\", \"test\"]:\n",
    "            base_labels_content = open(\"{}_base_labels_{}.txt\".format(args.name, typ), 'r').readlines()\n",
    "            base_labels[typ] = [int(label.strip()) for label in base_labels_content]\n",
    "            \n",
    "    debug = args.debug\n",
    "    if debug:\n",
    "        args.train_batch_size = 2\n",
    "        args.eval_batch_size = 2\n",
    "        args.num_train_epochs = 1\n",
    "\n",
    "    processors = {\n",
    "        \"sst-wiki\": SstWikiProcessor,\n",
    "        \"pronoun\": PronounProcessor\n",
    "    }\n",
    "\n",
    "    output_modes = {\n",
    "        \"sst-wiki\": \"classification\",\n",
    "        \"pronoun\": \"classification\",\n",
    "    }\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "    torch.manual_seed(params['seed'])\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(params['seed'])\n",
    "\n",
    "    if not args.do_train and not args.do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    input_processor_type = args.input_processor_type.lower()\n",
    "\n",
    "    if input_processor_type not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (input_processor_type))\n",
    "\n",
    "    processor = processors[input_processor_type]()\n",
    "    output_mode = output_modes[input_processor_type]\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    print(label_list)\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_optimization_steps = None\n",
    "    if args.do_train:\n",
    "        limit = 2 if debug else 0\n",
    "        train_examples = processor.get_train_examples(args.data_dir, limit)\n",
    "        num_train_optimization_steps = int(\n",
    "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "        if args.local_rank != -1:\n",
    "            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    # Prepare model\n",
    "    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "    model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "              cache_dir=cache_dir,\n",
    "              num_labels=num_labels)\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        model = DDP(model)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizerb\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FusedAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                              lr=args.learning_rate,\n",
    "                              bias_correction=False,\n",
    "                              max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0:\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "        warmup_linear = WarmupLinearSchedule(warmup=args.warmup_proportion,\n",
    "                                             t_total=num_train_optimization_steps)\n",
    "\n",
    "    else:\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=args.learning_rate,\n",
    "                             warmup=args.warmup_proportion,\n",
    "                             t_total=num_train_optimization_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    nb_tr_steps = 0\n",
    "    tr_loss = 0\n",
    "    if args.do_train:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "        print(\"typ\\tepoch\\tacc\\tavg_mean_mass\\tavg_max_mass\\tloss\\thammer_loss\\tlabel_match_score\\tavg_mean_vn\\tavg_max_vn\\tavg_min_vn\")\n",
    "        model.train()\n",
    "\n",
    "        for epoch in trange(int(args.num_train_epochs) + 1, desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            \n",
    "            if epoch > 0:\n",
    "                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "                    # define a new function to compute loss values for both output_modes\n",
    "                    logits, attention_probs_layers, category_mask, _ = model(input_ids, \n",
    "                                                                            token_type_ids=segment_ids,\n",
    "                                                                            pad_attention_mask=input_mask,\n",
    "                                                                            manipulate_attention=True,\n",
    "                                                                            category_mask=None,\n",
    "                                                                            labels=None)\n",
    "                    # logits - B x 2 \n",
    "                    loss_fct = CrossEntropyLoss() # averages the loss over B\n",
    "                    # print(label_ids)\n",
    "                    # # print(logits)\n",
    "                    loss =loss_fct(logits, label_ids)\n",
    "                    # print('attetnion prob layer')\n",
    "                    # print(attention_probs_layers)\n",
    "                    # loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "                    loss += attention_regularization_loss(attention_probs_layers, \n",
    "                                                            category_mask,\n",
    "                                                            input_mask,\n",
    "                                                            args.hammer_coeff, \n",
    "                                                            optimize_func=args.att_opt_func,\n",
    "                                                            debug=debug)[0]\n",
    "                    \n",
    "                    if n_gpu > 1:\n",
    "                        loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                    if args.gradient_accumulation_steps > 1:\n",
    "                        loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "                    if args.fp16:\n",
    "                        optimizer.backward(loss)\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "\n",
    "                    tr_loss += loss.item()\n",
    "                    nb_tr_examples += input_ids.size(0)\n",
    "                    nb_tr_steps += 1\n",
    "                    if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                        if args.fp16:\n",
    "                            # modify learning rate with special warm up BERT uses\n",
    "                            # if args.fp16 is False, BertAdam is used that handles this automatically\n",
    "                            lr_this_step = args.learning_rate * warmup_linear.get_lr(global_step/num_train_optimization_steps,\n",
    "                                                                                    args.warmup_proportion)\n",
    "                            for param_group in optimizer.param_groups:\n",
    "                                param_group['lr'] = lr_this_step\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        global_step += 1\n",
    "                    if debug:\n",
    "                        break\n",
    "\n",
    "            # EVALUATION AFTER EVERY EPOCH\n",
    "            eval_preds = {}\n",
    "            for typ in [\"dev\", \"test\"]:\n",
    "                eval_preds[typ] = run_evaluation(args, processor, label_list, tokenizer, output_mode, epoch, \n",
    "                                                    model, num_labels, tr_loss, global_step, device, input_processor_type, \n",
    "                                                    base_labels, debug, typ)\n",
    "\n",
    "            #dump labels after the last epoch, or when first_run\n",
    "            if args.first_run or epoch == args.num_train_epochs:\n",
    "                for typ in [\"dev\", \"test\"]:\n",
    "                    preds = eval_preds[typ]\n",
    "                    filename = \"{}_labels_{}_.txt\".format(typ, epoch)\n",
    "                    labels_file = os.path.join(args.output_dir, filename)\n",
    "                    with open(labels_file, \"w\") as writer:\n",
    "                        logger.info(\"Dumping labels in the file: {}\".format(labels_file))\n",
    "                        writer.write('\\n'.join([str(pred) for pred in preds]))\n",
    "\n",
    "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        # Save a trained model, configuration and tokenizer\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n",
    "        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        model_to_save.config.to_json_file(output_config_file)\n",
    "        tokenizer.save_vocabulary(args.output_dir)\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = BertForSequenceClassification.from_pretrained(args.output_dir, num_labels=num_labels)\n",
    "        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(args.bert_model, num_labels=num_labels)\n",
    "    model.to(device)\n",
    "\n",
    "def run_evaluation(args, processor, label_list, tokenizer, output_mode, epoch, \n",
    "                    model, num_labels, tr_loss, global_step, device, input_processor_type, \n",
    "                    base_labels, debug, typ=\"dev\"):\n",
    "    \n",
    "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        \n",
    "        limit = 2 if debug else 0\n",
    "        if typ == \"dev\":\n",
    "            eval_examples = processor.get_dev_examples(args.data_dir, limit)\n",
    "        else:\n",
    "            eval_examples = processor.get_test_examples(args.data_dir, limit)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n",
    "        logger.info(\"***** Running evaluation on \" + typ + \" data*****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        ce_eval_loss = 0\n",
    "        ar_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        preds = []\n",
    "        attmaxidx=[]\n",
    "        tms=[]\n",
    "        tmp_avg_attention_mass = 0.0\n",
    "        tmp_max_attention_mass = 0.0\n",
    "        tmp_max_attention_mass_non_reg = 0.0\n",
    "        tmp_non_reg_mass = 0.0\n",
    "        tmp_pad_mass = 0.0\n",
    "\n",
    "        tmp_vnfs = [0., 0., 0.]\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, attention_probs_layers, category_mask, vnfs = model(input_ids, \n",
    "                                                                        token_type_ids=segment_ids,\n",
    "                                                                        pad_attention_mask=input_mask,\n",
    "                                                                        manipulate_attention=True,\n",
    "                                                                        category_mask=None, \n",
    "                                                                        labels=None)\n",
    "\n",
    "            # create eval loss and other metric required by the task\n",
    "            loss_fct = CrossEntropyLoss() # averages the loss over B\n",
    "            tmp_ce_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "\n",
    "            tmp_ar_eval_loss, avg_attention_mass, non_reg_mass,total_mass, pad_mass, max_attention_mass,max_non_reg_mass,idxmaxloss = \\\n",
    "                attention_regularization_loss(attention_probs_layers, \n",
    "                                                category_mask,\n",
    "                                                input_mask,\n",
    "                                                args.hammer_coeff,\n",
    "                                                optimize_func=args.att_opt_func)\n",
    "            tmp_max_attention_mass_non_reg += max_non_reg_mass.item()\n",
    "            tmp_avg_attention_mass += avg_attention_mass.item()\n",
    "            tmp_max_attention_mass += max_attention_mass.item()\n",
    "            tmp_non_reg_mass += non_reg_mass.item()\n",
    "            tmp_pad_mass += pad_mass.item()\n",
    "se\n",
    "            tmp_vnfs = [tmp_vnfs[i] + vnfs[i] for i in range(3)]\n",
    "\n",
    "            tmp_eval_loss = tmp_ce_eval_loss + tmp_ar_eval_loss\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            ce_eval_loss += tmp_ce_eval_loss.mean().item()\n",
    "            ar_eval_loss += tmp_ar_eval_loss.mean().item()\n",
    "            attmaxidx.append(non_reg_mass.item())\n",
    "            print('reg attention sum per layer')\n",
    "            print(total_mass)\n",
    "            tms.append(total_mass)\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            if debug:\n",
    "                break\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        ce_eval_loss = ce_eval_loss / nb_eval_steps\n",
    "        ar_eval_loss = ar_eval_loss / nb_eval_steps\n",
    "        print('attetnion')\n",
    "        print(pd.Series(attmaxidx).describe())\n",
    "        # print(pd.Series(tms).describe())\n",
    "        # print(attmaxidx)\n",
    "        preds = preds[0]\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        result = compute_metrics(input_processor_type, preds, all_label_ids.numpy())\n",
    "        loss = tr_loss/global_step if (args.do_train and epoch > 0) else None\n",
    "\n",
    "        result['eval_loss'] = eval_loss\n",
    "        result['ce_eval_loss'] = ce_eval_loss\n",
    "        result['ar_eval_loss'] = ar_eval_loss\n",
    "\n",
    "        result['global_step'] = global_step\n",
    "        result['loss'] = loss\n",
    "\n",
    "        result['avg_mean_attention_mass'] = tmp_avg_attention_mass / nb_eval_steps\n",
    "        result['avg_max_attention_mass'] = tmp_max_attention_mass / nb_eval_steps\n",
    "        result['avg_non_reg_attention_mass'] = tmp_non_reg_mass / nb_eval_steps\n",
    "        result['avg_pad_attention_mass'] = tmp_pad_mass / nb_eval_steps\n",
    "        result['avg_max_attention_mass_non_reg'] =tmp_max_attention_mass_non_reg / nb_eval_steps\n",
    "        result['avg_mean_value_norm'] = tmp_vnfs[0]*1. / nb_eval_steps\n",
    "        result['avg_max_value_norm'] = tmp_vnfs[1]*1. / nb_eval_steps\n",
    "        result['avg_min_value_norm'] = tmp_vnfs[2]*1. / nb_eval_steps\n",
    "        result['attmaxidx']=attmaxidx\n",
    "\n",
    "        result['label_match_score'] = 0.0\n",
    "        if not args.first_run:\n",
    "            num_labels = len(preds)\n",
    "            result['label_match_score'] = simple_accuracy(preds, base_labels[typ][0:num_labels])\n",
    "\n",
    "        output_eval_file = os.path.join(args.output_dir, typ+ \"_\"+str(params['seed'])+ \"_\"+str(args.hammer_coeff) + \"_\"+str(epoch)+\"_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** {} results *****\".format(typ))\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        print('\\t'.join([ str(elem) for elem in \n",
    "                        [typ, \n",
    "                        epoch, \n",
    "                        result['acc'],\n",
    "                        # result['f1'],\n",
    "                        result['avg_mean_attention_mass'],\n",
    "                        result['avg_max_attention_mass'],\n",
    "                        result['avg_max_attention_mass_non_reg'],\n",
    "                        result['eval_loss'],\n",
    "                        result['ar_eval_loss'],\n",
    "                        result['label_match_score'],\n",
    "                        result['avg_mean_value_norm'], \n",
    "                        result['avg_max_value_norm'], \n",
    "                        result['avg_min_value_norm'] ,\n",
    "                        result['attmaxidx']\n",
    "                    ]]))\n",
    "#         print(preds)\n",
    "        return preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 05:54:01 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST RUN: True\n",
      "['1', '2', '3', '4', '5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 05:54:02 - INFO - pytorch_pretrained_bert_local.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\howar\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "01/29/2021 05:54:03 - INFO - pytorch_pretrained_bert_local.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\howar\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "01/29/2021 05:54:03 - INFO - pytorch_pretrained_bert_local.modeling -   extracting archive file C:\\Users\\howar\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\howar\\AppData\\Local\\Temp\\tmpdcte9ld3\n",
      "01/29/2021 05:54:06 - INFO - pytorch_pretrained_bert_local.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 05:54:08 - INFO - pytorch_pretrained_bert_local.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "01/29/2021 05:54:08 - INFO - pytorch_pretrained_bert_local.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   Writing example 0 of 7000\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   *** Example ***\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   guid: train-0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   tokens: [CLS] my wife took me here on my birthday for breakfast and it was excellent . the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure . our waitress was excellent and our food arrived quickly on the semi - busy saturday morning . it looked like the place fills up pretty quickly so the earlier you get here the better . do yourself a favor and get their bloody mary . it was phenomena ##l and simply the best i ' ve ever had . i ' m pretty sure they only use ingredients from their garden and blend them fresh when you order it . it was amazing . while everything on the menu looks excellent , i had the white tr ##uf ##fle scrambled eggs vegetable skill ##et and it was ta ##sty and delicious . it came with 2 pieces of their grid ##dled bread with was amazing and it absolutely made the meal complete . it was the best toast i ' ve ever had . anyway , i can ' t wait to go back ! [SEP] [SEP]\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_ids: 101 2026 2564 2165 2033 2182 2006 2026 5798 2005 6350 1998 2009 2001 6581 1012 1996 4633 2001 3819 2029 2081 3564 2648 12549 2037 5286 2019 7619 5165 1012 2256 13877 2001 6581 1998 2256 2833 3369 2855 2006 1996 4100 1011 5697 5095 2851 1012 2009 2246 2066 1996 2173 17469 2039 3492 2855 2061 1996 3041 2017 2131 2182 1996 2488 1012 2079 4426 1037 5684 1998 2131 2037 6703 2984 1012 2009 2001 13352 2140 1998 3432 1996 2190 1045 1005 2310 2412 2018 1012 1045 1005 1049 3492 2469 2027 2069 2224 12760 2013 2037 3871 1998 12586 2068 4840 2043 2017 2344 2009 1012 2009 2001 6429 1012 2096 2673 2006 1996 12183 3504 6581 1010 1045 2018 1996 2317 19817 16093 21031 13501 6763 15415 8066 3388 1998 2009 2001 11937 21756 1998 12090 1012 2009 2234 2007 1016 4109 1997 2037 8370 20043 7852 2007 2001 6429 1998 2009 7078 2081 1996 7954 3143 1012 2009 2001 1996 2190 15174 1045 1005 2310 2412 2018 1012 4312 1010 1045 2064 1005 1056 3524 2000 2175 2067 999 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   label: 5 (id = 4)\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   *** Example ***\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   guid: train-1\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   tokens: [CLS] i have no idea why some people give bad reviews about this place . it goes to show you , you can please everyone . they are probably grip ##ing about something that their own fault . . . there are many people like that . in any case , my friend and i arrived at about 5 : 50 pm this past sunday . it was pretty crowded , more than i thought for a sunday evening and thought we would have to wait forever to get a seat but they said we ' ll be seated when the girl comes back from seating someone else . we were seated at 5 : 52 and the waiter came and got our drink orders . everyone was very pleasant from the host that seated us to the waiter to the server . the prices were very good as well . we placed our orders once we decided what we wanted at 6 : 02 . we shared the baked spaghetti cal ##zone and the small here ' s the beef pizza so we can both try them . the cal ##zone was huge and we got the smallest one ( personal ) and got the small 11 pizza . both were awesome ! my friend liked the pizza better and i liked the cal ##zone better . the cal ##zone does have a sweet ##ish sauce but that ' s how i like my sauce ! we had to box part of the pizza to take it home and we were out the door by 6 : 42 . so , everything was great and not like these bad reviewers . that goes to show you that you have to try these things yourself because all these bad reviewers have some serious issues . [SEP] [SEP]\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_ids: 101 1045 2031 2053 2801 2339 2070 2111 2507 2919 4391 2055 2023 2173 1012 2009 3632 2000 2265 2017 1010 2017 2064 3531 3071 1012 2027 2024 2763 6218 2075 2055 2242 2008 2037 2219 6346 1012 1012 1012 2045 2024 2116 2111 2066 2008 1012 1999 2151 2553 1010 2026 2767 1998 1045 3369 2012 2055 1019 1024 2753 7610 2023 2627 4465 1012 2009 2001 3492 10789 1010 2062 2084 1045 2245 2005 1037 4465 3944 1998 2245 2057 2052 2031 2000 3524 5091 2000 2131 1037 2835 2021 2027 2056 2057 1005 2222 2022 8901 2043 1996 2611 3310 2067 2013 10747 2619 2842 1012 2057 2020 8901 2012 1019 1024 4720 1998 1996 15610 2234 1998 2288 2256 4392 4449 1012 3071 2001 2200 8242 2013 1996 3677 2008 8901 2149 2000 1996 15610 2000 1996 8241 1012 1996 7597 2020 2200 2204 2004 2092 1012 2057 2872 2256 4449 2320 2057 2787 2054 2057 2359 2012 1020 1024 6185 1012 2057 4207 1996 17776 26666 10250 15975 1998 1996 2235 2182 1005 1055 1996 12486 10733 2061 2057 2064 2119 3046 2068 1012 1996 10250 15975 2001 4121 1998 2057 2288 1996 10479 2028 1006 3167 1007 1998 2288 1996 2235 2340 10733 1012 2119 2020 12476 999 2026 2767 4669 1996 10733 2488 1998 1045 4669 1996 10250 15975 2488 1012 1996 10250 15975 2515 2031 1037 4086 4509 12901 2021 2008 1005 1055 2129 1045 2066 2026 12901 999 2057 2018 2000 3482 2112 1997 1996 10733 2000 2202 2009 2188 1998 2057 2020 2041 1996 2341 2011 1020 1024 4413 1012 2061 1010 2673 2001 2307 1998 2025 2066 2122 2919 15814 1012 2008 3632 2000 2265 2017 2008 2017 2031 2000 3046 2122 2477 4426 2138 2035 2122 2919 15814 2031 2070 3809 3314 1012 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 05:54:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   label: 5 (id = 4)\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   *** Example ***\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   guid: train-2\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   tokens: [CLS] love the g ##yr ##o plate . rice is so good and i also dig their candy selection : ) [SEP] [SEP]\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_ids: 101 2293 1996 1043 12541 2080 5127 1012 5785 2003 2061 2204 1998 1045 2036 10667 2037 9485 4989 1024 1007 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   segment_ids: 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   label: 4 (id = 3)\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   *** Example ***\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   guid: train-3\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   tokens: [CLS] rosie , dakota , and i love cha ##par ##ral dog park ! ! ! it ' s very convenient and surrounded by a lot of paths , a desert x ##eri ##sca ##pe , baseball fields , ball ##park ##s , and a lake with ducks . the scott ##sdale park and rec dept . does a wonderful job of keeping the park clean and shaded . you can find trash cans and po ##op ##y - pick up mit ##ts located all over the park and paths . the fence ##d in area is huge to let the dogs run , play , and sniff ! [SEP] [SEP]\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_ids: 101 15820 1010 7734 1010 1998 1045 2293 15775 19362 7941 3899 2380 999 999 999 2009 1005 1055 2200 14057 1998 5129 2011 1037 2843 1997 10425 1010 1037 5532 1060 11124 15782 5051 1010 3598 4249 1010 3608 14432 2015 1010 1998 1037 2697 2007 14875 1012 1996 3660 15145 2380 1998 28667 29466 1012 2515 1037 6919 3105 1997 4363 1996 2380 4550 1998 25273 1012 2017 2064 2424 11669 18484 1998 13433 7361 2100 1011 4060 2039 10210 3215 2284 2035 2058 1996 2380 1998 10425 1012 1996 8638 2094 1999 2181 2003 4121 2000 2292 1996 6077 2448 1010 2377 1010 1998 27907 999 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 05:54:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   label: 5 (id = 4)\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   *** Example ***\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   guid: train-4\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   tokens: [CLS] general manager scott pete ##llo is a good egg ! ! ! not to go into detail , but let me assure you if you have any issues ( albeit rare ) speak with scott and treat the guy with some respect as you state your case and i ' d be surprised if you don ' t walk out totally satisfied as i just did . like i always say . . . . . mistakes are inevitable , it ' s how we recover from them that is important ! ! ! thanks to scott and his awesome staff . you ' ve got a customer for life ! ! . . . . . . . . . . : ^ ) [SEP] [SEP]\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_ids: 101 2236 3208 3660 6969 7174 2003 1037 2204 8288 999 999 999 2025 2000 2175 2046 6987 1010 2021 2292 2033 14306 2017 2065 2017 2031 2151 3314 1006 12167 4678 1007 3713 2007 3660 1998 7438 1996 3124 2007 2070 4847 2004 2017 2110 2115 2553 1998 1045 1005 1040 2022 4527 2065 2017 2123 1005 1056 3328 2041 6135 8510 2004 1045 2074 2106 1012 2066 1045 2467 2360 1012 1012 1012 1012 1012 12051 2024 13418 1010 2009 1005 1055 2129 2057 8980 2013 2068 2008 2003 2590 999 999 999 4283 2000 3660 1998 2010 12476 3095 1012 2017 1005 2310 2288 1037 8013 2005 2166 999 999 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1024 1034 1007 102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/29/2021 05:54:10 - INFO - __main__ -   label: 5 (id = 4)\n"
     ]
    }
   ],
   "source": [
    "params={}\n",
    "odir='twitnotebook'\n",
    "for seed in range(1,5,1):\n",
    "    for hammer_coeff in [0.2,0,0.1,1.0]:\n",
    "        for opt_func in ['mean','max']:\n",
    "            params['output_dir']=odir\n",
    "            params['hammer_coeff']=hammer_coeff\n",
    "            params['seed']=seed\n",
    "            params['optfunc']=opt_func\n",
    "            main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mdl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
